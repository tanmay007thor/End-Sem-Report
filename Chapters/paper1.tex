
\section{Methodology}

We begin by importing the Sleep-EDF data provided in EDF (European Data Format). The raw EEG recordings are loaded using MNE's \texttt{read\_raw\_edf} function, and corresponding annotations are aligned using \texttt{read\_annotations}. We optionally exclude all non-EEG channels unless explicitly required, focusing on EEG signals for our analysis. The dataset includes two EEG channels: \texttt{Fpz-Cz} and \texttt{Pz-Oz}, which are the primary input sources in our work.

During loading, we crop the signal to reduce unnecessary wake-time data, retaining 30 minutes before the first sleep stage and 30 minutes after the last. This ensures that our training data remains focused around relevant sleep activity. The channel names are cleaned by stripping prefixes for standardization.

After loading and cropping, we segment the data into fixed-length 30-second epochs using the known sampling frequency of the recordings. Only epochs labeled with valid sleep stages — specifically stages W, N1, N2, N3, and REM — are retained. For each epoch, the raw data is extracted and structured into a format suitable for training.

Once all epochs are collected, we flatten the 3D EEG data (epochs $\times$ channels $\times$ time points) into 2D arrays (epochs $\times$ features). This is necessary for traditional machine learning classifiers. To address class imbalance in sleep stages, we apply SMOTE (Synthetic Minority Oversampling Technique) to generate balanced training samples.


 


The balanced dataset is then split into training and testing sets in an 80-20 ratio. We train a  Machine Learning   on the training data. After training, predictions are made on the test set, and the model is evaluated using standard metrics including classification report and confusion matrix. The confusion matrix is visualized using a heatmap to give a clear view of the stage-wise performance of the classifier.


\section{Dataset Information}

We have used the Sleep-EDF Dataset available at \url{https://www.physionet.org/content/sleep-edfx/1.0.0/}. For our experiments, we selected recordings from approximately 30 patients, resulting in 60 files — 30 EDF (European Data Format) recordings and their corresponding hypnogram annotation files. The EDF files include various bio-signals captured through multiple channels. The hypnogram files contain sleep stage annotations, specifying the start time and type of each stage (Wake, N1, N2, N3, N4, REM) with timestamps in the HH:MM:SS format.

The EDF recordings consist of a range of physiological signals, including EEG (\texttt{Fpz-Cz} and \texttt{Pz-Oz}), EOG, EMG (submental), rectal temperature, and oro-nasal respiration. Each channel provides continuous signal recordings at a specific sampling frequency (Hz), which are used as input features for our downstream machine learning pipeline. From these signals, we derive two essential inputs: spatial features, used to construct graph-based representations, and temporal features, which capture the sequence and timing dynamics necessary for modeling the sleep process.


\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{"./img/paper_1/EEG_Filter_chanel.png"}
	\caption{Filter EEG Chanels}
	\label{fig:eegfilterchanel}
\end{figure}


\section{Preprocessing}

This study applied machine learning techniques to classify sleep stages using EEG signals. The preprocessing pipeline included several crucial steps: channel mapping, data cropping, signal filtering, epoch creation, label mapping, and class balancing using SMOTE. These steps ensured clean, well-structured input for feature extraction and model training. The extracted features spanned time-domain, frequency-domain, and time-frequency representations. Multiple machine learning models—Gradient Boosting, Random Forest, Support Vector Machine (SVC), and Bagging—were trained and evaluated using metrics such as accuracy, precision, recall, and F1-score with cross-validation to assess generalization. The trained models were finally used to classify unseen EEG data into five sleep stages: Wake, N1, N2, N3, and REM.

\paragraph{A. Dataset Acquisition}

EEG signals were recorded using non-invasive electrodes placed on the scalp of human volunteers, capturing brain electrical activity during overnight sleep studies. Additional demographic data such as age, gender, medical history, and sleep habits were collected to provide contextual understanding. The data collection took place in controlled conditions to minimize noise and interference. Manual inspection of the raw EEG was performed to reject or correct artifacts where necessary, ensuring the quality of the input data. The final recordings were saved in standard EDF format for compatibility with downstream analysis. The dataset used in this study was sourced from publicly available Sleep-EDF recordings, which include EEG data and associated hypnogram annotations reflecting sleep stages.

\paragraph{B. Data Preprocessing}

Preprocessing was applied to prepare EEG data for modeling. Channel mapping was done to identify relevant EEG channels, particularly \texttt{Fpz-Cz} and \texttt{Pz-Oz}, and irrelevant or noisy channels were excluded. Data cropping was applied to isolate the sleep period from wake periods based on annotation markers. The signals were then filtered to remove noise and isolate specific frequency bands important for sleep analysis.

Epochs of 30-second duration were created from continuous EEG signals to structure the data for machine learning. Sleep stage labels corresponding to these epochs were derived from hypnogram annotations. Given the naturally imbalanced class distribution in sleep stage data (e.g., fewer REM and N1 stages), the SMOTE algorithm was employed to oversample the minority classes, thereby enhancing model robustness. After preprocessing, the dataset was split into training and testing subsets. This enabled reliable training of models and unbiased performance evaluation. The output from preprocessing was then used in the feature extraction and modeling stages.



 
  
	



\begin{algorithm}
	\textbf{Input:} raw\_fname, annot\_fname, load\_eeg\_only, crop\_wake\_mins\\
	\textbf{Output:} raw\\
	
	mapping $\gets$ \{'EOG horizontal': 'eog', \dots\}\\
	exclude $\gets$ mapping.keys() \textbf{if} load\_eeg\_only \textbf{then do nothing}\\
	raw $\gets$ read\_raw\_edf(raw\_fname, exclude)\\
	annots $\gets$ read\_annotations(annot\_fname)\\
	raw.set\_annotations(annots)\\
	\textbf{if} not load\_eeg\_only \textbf{then}\\
	\hspace{1cm} raw.set\_channel\_types(mapping)\\
	\textbf{if} crop\_wake\_mins > 0 \textbf{then}\\
	\hspace{1cm} mask $\gets$ [x[-1] in \{'1','2','3','4','R'\} for x in annots.description]\\
	\hspace{1cm} inds $\gets$ where(mask)\\
	\hspace{1cm} tmin $\gets$ annots[inds[0]].onset - crop\_wake\_mins * 60\\
	\hspace{1cm} tmax $\gets$ annots[inds[-1]].onset + crop\_wake\_mins * 60\\
	\hspace{1cm} raw.crop(tmin, tmax)\\
	rename EEG channel names\\
	subj\_nb $\gets$ int(fname[3:5]), rec\_nb $\gets$ int(fname[5])\\
	raw.info.subject\_info $\gets$ \{id: subj\_nb, rec\_id: rec\_nb\}\\
	\textbf{Return} raw\\
	\caption{load\_sleep\_physionet\_raw}
\end{algorithm}

\begin{algorithm}
	\textbf{Input:} fnames, l\_freq, h\_freq\\
	\textbf{Output:} raws\\
	
	raws $\gets$ [load\_sleep\_physionet\_raw(f[0], f[1]) for f in fnames]\\
	\textbf{for each} raw \textbf{in} raws \textbf{do}\\
	\hspace{1cm} raw.load\_data()\\
	\hspace{1cm} raw.filter(l\_freq, h\_freq)\\
	\textbf{Return} raws\\
	\caption{Load and Filter All Files}
\end{algorithm}

\begin{algorithm}
	\textbf{Input:} raw\_data\_list, epoch\_length = 30\\
	\textbf{Output:} epochs\_list, labels\\
	
	epochs\_list $\gets$ [\,]\\
	labels $\gets$ [\,]\\
	\textbf{for each} raw \textbf{in} raw\_data\_list \textbf{do}\\
	\hspace{1cm} sfreq $\gets$ int(raw.info['sfreq'])\\
	\hspace{1cm} (events, event\_ids) $\gets$ events\_from\_annotations(raw)\\
	\hspace{1cm} sleep\_ids $\gets$ [1,2,3,4,5]\\
	\hspace{1cm} filtered\_events $\gets$ [event for event in events if event[2] in sleep\_ids]\\
	\hspace{1cm} filtered\_event\_ids $\gets$ \{k:v for k,v in event\_ids.items() if v in sleep\_ids\}\\
	\hspace{1cm} filtered\_events $\gets$ np.array(filtered\_events, dtype=int)\\
	\hspace{1cm} epochs $\gets$ Epochs(raw, filtered\_events, filtered\_event\_ids, 0, epoch\_length - 1/sfreq)\\
	\hspace{1cm} epochs\_list.append(epochs)\\
	\hspace{1cm} labels.extend([event[2] for event in filtered\_events])\\
	\textbf{Return} epochs\_list, labels\\
	\caption{Epoch Creation}
\end{algorithm}

\begin{algorithm}
	\textbf{Input:} epochs\_list, labels\\
	\textbf{Output:} model performance metrics\\
	
	(epochs\_list, labels) $\gets$ create\_epochs(raws)\\
	X $\gets$ concatenate([epochs.get\_data() for epochs in epochs\_list], axis=0)\\
	(n\_epochs, n\_channels, n\_times) $\gets$ X.shape\\
	X $\gets$ reshape(X, (n\_epochs, n\_channels * n\_times))\\
	y $\gets$ np.array(labels)\\
	(X\_resampled, y\_resampled) $\gets$ SMOTE().fit\_resample(X, y)\\
	(X\_train, X\_test, y\_train, y\_test) $\gets$ train\_test\_split(X\_resampled, y\_resampled, 0.2)\\
	model $\gets$ initialize\_ML\_model()\\
	model.fit(X\_train, y\_train)\\
	y\_pred $\gets$ model.predict(X\_test)\\
	report $\gets$ classification\_report(y\_test, y\_pred)\\
	cm $\gets$ confusion\_matrix(y\_test, y\_pred)\\
	display(report)\\
	display\_confusion\_matrix(cm)\\
	\caption{ML Training and Evaluation Pipeline}
\end{algorithm}

	
	
	
	\begin{algorithm}
		\textbf{Input:} list of (signal\_file, annotation\_file) pairs \\
		\textbf{Output:} average accuracy, classification report, confusion matrix
		
		\vspace{0.3cm}
		\texttt{epochs\_list = []} \\
		\texttt{for file\_pair in fnames:} \\
		\hspace*{0.5cm}\texttt{raw = read\_raw\_edf(file\_pair[0])} \\
		\hspace*{0.5cm}\texttt{annot = read\_annotations(file\_pair[1])} \\
		\hspace*{0.5cm}\texttt{raw.set\_annotations(annot)} \\
		\hspace*{0.5cm}\texttt{annot.crop(annot[1].onset - 1800, annot[-2].onset + 1800)} \\
		\hspace*{0.5cm}\texttt{events = events\_from\_annotations(raw)} \\
		\hspace*{0.5cm}\texttt{epochs = Epochs(raw, events)} \\
		\hspace*{0.5cm}\texttt{epochs\_list.append(epochs)} \\
		
		\vspace{0.3cm}
		\texttt{all\_epochs = concatenate\_epochs(epochs\_list)} \\
		\texttt{y = all\_epochs.events[:, 2]} \\
		
		\vspace{0.3cm}
		\textbf{Define feature extraction:} \\
		\texttt{def eeg\_power\_band(epochs):} \\
		\hspace*{0.5cm}\texttt{psds = compute\_psd(epochs)} \\
		\hspace*{0.5cm}\texttt{psds = normalize(psds)} \\
		\hspace*{0.5cm}\texttt{features = []} \\
		\hspace*{0.5cm}\texttt{for band in freq\_bands:} \\
		\hspace*{1cm}\texttt{features.append(mean\_psd\_in\_band)} \\
		\hspace*{0.5cm}\texttt{return concatenate(features)} \\
		
		\vspace{0.3cm}
		\textbf{Train ML pipeline with KFold:} \\
		\texttt{pipeline = [eeg\_power\_band, Classifier()]} \\
		\texttt{kf = KFold(n\_splits=5)} \\
		\texttt{results = \{\}} \\
		\texttt{for train\_idx, test\_idx in kf.split(all\_epochs):} \\
		\hspace*{0.5cm}\texttt{X\_train = all\_epochs[train\_idx]} \\
		\hspace*{0.5cm}\texttt{X\_test = all\_epochs[test\_idx]} \\
		\hspace*{0.5cm}\texttt{y\_train = y[train\_idx]} \\
		\hspace*{0.5cm}\texttt{y\_test = y[test\_idx]} \\
		\hspace*{0.5cm}\texttt{pipeline.fit(X\_train, y\_train)} \\
		\hspace*{0.5cm}\texttt{y\_pred = pipeline.predict(X\_test)} \\
		\hspace*{0.5cm}\texttt{store accuracy, y\_true, y\_pred} \\
		
	 
		
		\caption{General ML Pipeline with K-Fold Cross-Validation}
	\end{algorithm}
	
	
	
	
	
	
 
 
 
 
 
 
 
 
 
 
 
 
 
 \RestyleAlgo{ruled}
 \SetKwComment{Comment}{/* }{ */}
 \DontPrintSemicolon
 
 \begin{algorithm*}[!t]
 	\caption{Main Pipeline for Sleep Stage Classification}
 	\label{alg:main}
 	
 	\KwData{Raw EEG and annotation file paths}
 	\KwResult{Trained SleepGCN\_Transformer model}
 	
 	fnames $\gets$ \texttt{[('SC4001E0-PSG.edf', 'SC4001EC-Hypnogram.edf')]} \;
 	$(X_{\text{all}}, Y_{\text{all}}) \gets$ \texttt{preprocess\_data(fnames)} \;
 	$X_{\text{tensor}} \gets \texttt{torch.tensor}(X_{\text{all}}, \texttt{float32})$ \;
 	$Y_{\text{tensor}} \gets \texttt{torch.tensor}(Y_{\text{all}}, \texttt{long})$ \;
 	
 	dataset $\gets$ \texttt{SleepEEGDataset}($X_{\text{tensor}}, Y_{\text{tensor}}$) \;
 	$train\_size \gets 0.8 \times \texttt{len}(dataset)$ \;
 	$val\_size \gets \texttt{len}(dataset) - train\_size$ \;
 	
 	$(train\_dataset, val\_dataset) \gets$ \texttt{random\_split}(dataset, [train\_size, val\_size]) \;
 	
 	model $\gets$ \texttt{SleepGCN\_Transformer}(\;
 	\Indp
 	input\_dim=4, hidden\_dim=256, output\_dim=5,\;
 	num\_layers=5, nhead=4, dropout=0.1)\;
 	\Indm
 	
 	trainer $\gets$ \texttt{SleepTrainer}(\;
 	\Indp
 	model, train\_dataset, val\_dataset,\;
 	batch\_size=32, lr=0.0003,\;
 	num\_epochs=20, weight\_decay=1e-4)\;
 	\Indm
 	
 	\texttt{trainer.train()} \;
 	
 \end{algorithm*}
 
 
 
 
 
 \RestyleAlgo{ruled}
 
 %% This is needed if you want to add comments in
 %% your algorithm with \Comment
 \SetKwComment{Comment}{/* }{ */}
 
 \begin{algorithm*}[!t]
 	\caption{SleepEEGDataset Class Definition}
 	\label{alg:sleepdataset}
 	
 	\KwData{Input features $X$, labels $Y$}
 	\KwResult{A graph-based dataset with node features and edge structure}
 	
 	\textbf{class} SleepEEGDataset(Dataset): \\
 	\Indp
 	\textbf{def} \_\_init\_\_(self, X, Y): \\
 	\Indp
 	self.X = X \\
 	self.Y = Y \\
 	A = np.array([ \\
 	
 	]) \\
 	edge\_index = torch.tensor(np.array(np.nonzero(A)), dtype=torch.long) \\
 	edge\_weights = torch.tensor(A[A.nonzero()], dtype=torch.float32) \\
 	self.edge\_index = edge\_index \\
 	self.edge\_weights = edge\_weights \\
 	\Indm
 	
 	\BlankLine
 	
 	\textbf{def} len(self): \\
 	\Indp
 	\textbf{return} len(self.X) \\
 	\Indm
 	
 	\BlankLine
 	
 	\textbf{def} get(self, idx): \\
 	\Indp
 	
 	label = torch.tensor([self.Y[idx]], dtype=torch.long) \\
 	\textbf{return} Data(x=node\_features, edge\_index=self.edge\_index, \\
 	\hspace{6em}edge\_attr=self.edge\_weights, y=label) \\
 	\Indm
 \end{algorithm*}
 
 
 
 
 
 
 
 
 
 
 \RestyleAlgo{ruled}
 \SetKwComment{Comment}{/* }{ */}
 
 \begin{algorithm*}[!t]
 	\caption{Preprocess Sleep EEG Data}\label{alg:preprocess_data}
 	\KwData{List of file pairs: $(psg\_file, ann\_file)$}
 	\KwResult{Preprocessed feature matrix $X$ and labels $Y$}
 	$X\_all \gets []$\;
 	$Y\_all \gets []$\;
 	\ForEach{$(psg\_file, ann\_file)$ in fnames}{ 
 		$raw \gets \text{mne.io.read\_raw\_edf}(psg\_file, preload=True)$\;
 		$annotations \gets \text{mne.read\_annotations}(ann\_file)$\;
 		$raw.set\_annotations(annotations)$\;
 		$raw.pick\_channels(channels)$\;
 		$raw.filter(0.3, 30., fir\_design='firwin')$\;
 		$data \gets raw.get\_data()$\;
 		$sfreq \gets raw.info['sfreq']$\;
 		
 		$epochs\_list \gets []$\;
 		$labels\_list \gets []$\;
 		
 		\ForEach{$annot$ in annotations}{
 			$stage \gets annot['description']$\;
 			\If{$stage \in stage\_mapping$}{
 				$onset\_sample \gets \text{int}(annot['onset'] \times sfreq)$\;
 				$duration\_samples \gets \text{int}(annot['duration'] \times sfreq)$\;
 				$epoch\_length\_samples \gets \text{int}(30 \times sfreq)$\;
 				
 				\For{$i \gets 0$ \KwTo $duration\_samples$ with step $epoch\_length\_samples$}{
 					$start \gets onset\_sample + i$\;
 					$stop \gets start + epoch\_length\_samples$\;
 					\If{$stop > data.shape[1]$}{
 						\textbf{break}\;
 					}
 					$epochs\_list.append(data[:, start:stop])$\;
 					$labels\_list.append(stage\_mapping[stage])$\;
 				}
 			}
 		}
 		
 		$X \gets \text{np.array}(epochs\_list)$\;
 		$Y \gets \text{np.array}(labels\_list)$\;
 		$X\_all.append(X)$\;
 		$Y\_all.append(Y)$\;
 	}
 	$X\_all \gets \texttt{np.concatenate}(X\_all, axis=0)$\;
 	$Y\_all \gets \texttt{np.concatenate}(Y\_all, axis=0)$\;
 	\KwRet{$X\_all, Y\_all$}
 \end{algorithm*}
 
 
 
 
 \RestyleAlgo{ruled}
 \SetKwComment{Comment}{/* }{ */}
 
 \begin{algorithm*}[!t]
 	\caption{Training Pipeline for Sleep EEG Model}\label{alg:training_pipeline}
 	\KwData{Training and Validation datasets, Model, Learning rate, Epochs, Batch size, Weight decay}
 	\KwResult{Trained model with performance metrics on training and validation data}
 	\SetKwFunction{FocalLoss}{FocalLoss}
 	\SetKwFunction{SleepTrainer}{SleepTrainer}
 	\SetKwFunction{Train}{train}
 	\SetKwFunction{GetClassWeights}{get\_class\_weights}
 	\SetKwFunction{ComputeAccuracy}{compute\_accuracy}
 	
 	\BlankLine
 	
 	\SetKwProg{Fn}{Function}{:}{}
 	\Fn{\FocalLoss{logits, targets}}{
 		probs $\gets$ \texttt{F.softmax(logits, dim=-1)}\;
 		targets\_one\_hot $\gets$ \texttt{F.one\_hot(targets, num\_classes=logits.shape[1]).float()}\;
 		targets\_one\_hot $\gets$ targets\_one\_hot * (1 - \texttt{label\_smoothing}) + \texttt{label\_smoothing} / logits.shape[1]\;
 		ce\_loss $\gets$ -targets\_one\_hot * \texttt{torch.log(probs + 1e-9)}\;
 		focal\_loss $\gets$ \texttt{alpha * (1 - probs) ** gamma * ce\_loss}\;
 		\KwRet \texttt{focal\_loss.mean()}
 	}
 	
 	\Fn{\SleepTrainer{model, train\_dataset, val\_dataset, batch\_size, lr, num\_epochs, weight\_decay}}{
 		class\_weights $\gets$ \texttt{self.get\_class\_weights(train\_dataset).to(device)}\;
 		loss\_fn $\gets$ \texttt{FocalLoss(alpha=class\_weights, gamma=3, label\_smoothing=0.1)}\;
 		optimizer $\gets$ \texttt{optim.AdamW(model.parameters(), lr=lr, weight\_decay=weight\_decay)}\;
 		scheduler $\gets$ \texttt{optim.lr\_scheduler.CosineAnnealingLR(optimizer, T\_max=num\_epochs)}\;
 		train\_loader $\gets$ \texttt{DataLoader(train\_dataset, batch\_size=batch\_size, shuffle=True)}\;
 		val\_loader $\gets$ \texttt{DataLoader(val\_dataset, batch\_size=batch\_size, shuffle=False)}\;
 		
 		\For{$epoch \gets 1$ \KwTo num\_epochs}{
 			model.train()\;
 			total\_train\_loss, total\_train\_accuracy $\gets$ 0, 0\;
 			
 			\ForEach{$batch$ in train\_loader}{
 				batch $\gets$ batch.to(device)\;
 				optimizer.zero\_grad()\;
 				output $\gets$ model(batch)\;
 				target $\gets$ batch.y\;
 				loss $\gets$ loss\_fn(output, target)\;
 				loss.backward()\;
 				optimizer.step()\;
 				total\_train\_loss += loss.item()\;
 				total\_train\_accuracy += compute\_accuracy(output, target)\;
 			}
 			
 			scheduler.step()\;
 			
 			\textbf{Validation Phase:}\;
 			model.eval()\;
 			total\_val\_loss, total\_val\_accuracy $\gets$ 0, 0\;
 			\ForEach{$batch$ in val\_loader}{
 				batch $\gets$ batch.to(device)\;
 				output $\gets$ model(batch)\;
 				target $\gets$ batch.y\;
 				loss $\gets$ loss\_fn(output, target)\;
 				total\_val\_loss += loss.item()\;
 				total\_val\_accuracy += compute\_accuracy(output, target)\;
 			}
 			
 			avg\_train\_loss $\gets$ total\_train\_loss / len(train\_loader)\;
 			avg\_train\_accuracy $\gets$ total\_train\_accuracy / len(train\_loader)\;
 			avg\_val\_loss $\gets$ total\_val\_loss / len(val\_loader)\;
 			avg\_val\_accuracy $\gets$ total\_val\_accuracy / len(val\_loader)\;
 			
 			print(f"Epoch {epoch}/{num\_epochs}: Train Loss: {avg\_train\_loss:.4f}, Train Acc: {avg\_train\_accuracy:.4f} | Val Loss: {avg\_val\_loss:.4f}, Val Acc: {avg\_val\_accuracy:.4f}")
 		}
 	}
 	\KwRet{Trained Model}
 \end{algorithm*}
 
 
 
 
 \RestyleAlgo{ruled}
 \SetKwComment{Comment}{/* }{ */}
 
 \begin{algorithm*}[!t]
 	\caption{SleepGCN\_Transformer Model Declaration}\label{alg:sleep_gcn_transformer}
 	\KwData{Input features of shape (batch size, input\_dim), graph structure with edges (edge\_index), batch information (batch)}
 	\KwResult{Predicted outputs for each input, of shape (batch size, output\_dim)}
 	\SetKwFunction{SleepGCNTransformer}{SleepGCN\_Transformer}
 	\SetKwFunction{Forward}{forward}
 	
 	\BlankLine
 	
 	\SetKwProg{Fn}{Function}{:}{}
 	\Fn{\SleepGCNTransformer{input\_dim, hidden\_dim, output\_dim, num\_layers, nhead, dropout}}{
 		\texttt{gcn\_layers} $\gets$ \texttt{nn.ModuleList()}\;
 		\texttt{batch\_norms} $\gets$ \texttt{nn.ModuleList()}\;
 		\texttt{gcn\_layers.append(GCNConv(input\_dim, hidden\_dim))}\;
 		\texttt{batch\_norms.append(BatchNorm(hidden\_dim))}\;
 		\For{$i \gets 1$ \KwTo num\_layers - 1}{
 			\texttt{gcn\_layers.append(GCNConv(hidden\_dim, hidden\_dim))}\;
 			\texttt{batch\_norms.append(BatchNorm(hidden\_dim))}\;
 		}
 		\texttt{encoder\_layer} $\gets$ \texttt{TransformerEncoderLayer(d\_model=hidden\_dim, nhead=nhead, dropout=dropout, batch\_first=True)}\;
 		\texttt{transformer} $\gets$ \texttt{TransformerEncoder(encoder\_layer, num\_layers=2)}\;
 		\texttt{fc} $\gets$ \texttt{nn.Linear(hidden\_dim, output\_dim)}\;
 		\texttt{activation} $\gets$ \texttt{ReLU()}\;
 		\texttt{dropout} $\gets$ \texttt{Dropout(dropout)}\;
 	}
 	
 	\BlankLine
 	
 	\Fn{\Forward{data}}{
 		\texttt{X, edge\_index, batch} $\gets$ \texttt{data.x, data.edge\_index, data.batch}\;
 		\For{$i \gets 0$ \KwTo len(gcn\_layers) - 1}{
 			\texttt{X} $\gets$ \texttt{activation(gcn\_layers[i](X, edge\_index))}\;
 			\texttt{X} $\gets$ \texttt{batch\_norms[i](X)}\;
 			\texttt{X} $\gets$ \texttt{dropout(X)}\;
 		}
 		\texttt{X} $\gets$ \texttt{global\_mean\_pool(X, batch)}\;
 		\texttt{X} $\gets$ \texttt{X.unsqueeze(1)}\;
 		\texttt{X} $\gets$ \texttt{transformer(X)}\;
 		\texttt{X} $\gets$ \texttt{X.squeeze(1)}\;
 		\KwRet \texttt{fc(X)}\;
 	}
 \end{algorithm*}
 
 
 
 
 
 
 




\section{Model Architecture and Learning Framework}


The core modeling pipeline begins with transforming the raw EEG data into structured epochs suitable for machine learning classification. Each EEG recording is segmented into 30-second epochs using the sampling frequency and annotated sleep stages. Only valid sleep stages (Wake, N1, N2, N3, and REM) are retained for downstream processing. After epoching, each segment is extracted using the \texttt{mne.Epochs} function, resulting in structured EEG data with consistent time windows. These segments are then reshaped into two-dimensional feature vectors where each row corresponds to an epoch and each column to a time-series sample across all EEG channels.

To address the class imbalance inherent in sleep stage data, the SMOTE (Synthetic Minority Over-sampling Technique) algorithm is applied. This balances the dataset by synthetically generating new examples in underrepresented classes. The balanced dataset is then split into training and testing sets in an 80:20 ratio.  

\begin{figure}
	\centering
	\includegraphics[width=0.9\linewidth]{"img/paper_1/Architechture1"}
	\caption{Architecture of Machine Learning Classifiers for Sleep Stage
		Classificatio}
	\label{fig:architechture1}
\end{figure}









\section{K-Fold Cross-Validation}

K-Fold Cross Validation  5-fold cross-validation approach was employed to evaluate the model's performance. The 5-fold cross-validation procedure divides the dataset into 5 equal subsets (or folds). In each iteration, the model is trained on 80\% of the data (four folds) and tested on the remaining 20\% (the fifth fold). This process is repeated 5 times, with each fold serving as the test set once, ensuring that each data point is tested exactly once.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.6\linewidth]{"img/paper_1/k-FOLD.PNG"}
	\caption{Architecture of Machine Learning Classifiers for Sleep Stage
		Classificatio}
	\label{fig:architechture1}
\end{figure}







This approach provides a more reliable estimate of model performance by reducing the potential bias that can arise from using a single train-test split. The final performance of the model is obtained by averaging the results from each fold, offering a comprehensive evaluation of its ability to generalize to unseen data.

The advantage of K-fold cross-validation lies in its ability to provide a more robust and generalized estimate of model performance by using different subsets for training and testing in each iteration. This process reduces the risk of overfitting and ensures that the model's evaluation is based on multiple different test sets.

\subsection{Random Forest with K-Fold Cross-Validation Pipeline}

The model pipeline was created using \texttt{make\_pipeline} from \texttt{sklearn.pipeline}, which integrates two main components: a function transformer and a random forest classifier. The function transformer is applied to extract the EEG power bands from the data, which are then used as features for training the model. The \texttt{RandomForestClassifier} was chosen for its ability to perform well with high-dimensional data, and it was set with 100 estimators to ensure robust performance. The random forest classifier was trained and tested on each fold using the 5-fold cross-validation setup, providing an unbiased evaluation of the model’s performance.



\subsection{Ensemble Classifier with K-Fold Cross-Validation Pipeline}

The pipeline was constructed using \texttt{make\_pipeline} from \texttt{sklearn.pipeline}, which integrates a function transformer for extracting EEG power bands as features and an ensemble classifier for prediction. The function transformer applies the \texttt{eeg\_power\_band} function to preprocess the data, and the \texttt{VotingClassifier} combines the predictions from a \texttt{RandomForestClassifier} and  The ensemble classifier was trained and tested on each fold using the 5-fold cross-validation setup, offering a more robust evaluation by leveraging the strengths of both classifiers.





\subsection{Gradient Boosting with K-Fold Cross-Validation Pipeline}
In this section, we employed Gradient Boosting with 5-fold cross-validation to classify the EEG data. Gradient Boosting is an ensemble learning technique that builds models sequentially, where each model attempts to correct the errors made by the previous ones. This method can improve predictive performance by focusing on difficult-to-predict instances. The dataset was split into five subsets, and each subset was used as the test set once, while the remaining four were used for training. The model's generalization ability was thus thoroughly evaluated.

A pipeline was constructed using \texttt{make\_pipeline} from \texttt{sklearn.pipeline}. It incorporates a function transformer that preprocesses the EEG data by extracting power band features through the \texttt{eeg\_power\_band} function. The processed data is then passed to a \texttt{GradientBoostingClassifier}, which is used for prediction. The \texttt{KFold} cross-validation technique with 5 splits ensures that the model is trained and validated on different portions of the dataset, offering a reliable estimate of its performance.












\subsection{Random Forest Classifier}

We implemented a Random Forest classifier to establish a strong baseline model for sleep stage classification. The raw EEG recordings were segmented into 30-second epochs, and features were extracted by flattening the multidimensional epoch data. To handle class imbalance, the SMOTE algorithm was applied prior to training. The model was initialized with \textbf{100 decision trees} (\texttt{n\_estimators=100}) and a fixed \textbf{random seed} (\texttt{random\_state=42}) to ensure reproducibility. This configuration allowed the model to generalize well while maintaining robust performance across varying sleep stages.

\subsection{Gradient Boosting Classifier with PCA}

To improve computational efficiency and potentially boost model accuracy, we introduced Principal Component Analysis (PCA), retaining \textbf{95\% of the explained variance} (\texttt{n\_components=0.95}). The reduced feature set was passed to a \textbf{Gradient Boosting classifier} configured with \textbf{30 estimators} (\texttt{n\_estimators=30}), a \textbf{maximum tree depth of 3} (\texttt{max\_depth=3}), and a \textbf{learning rate of 0.1}. Class balancing was again addressed with SMOTE. The same 80-20 train-test split was maintained with \texttt{random\_state=42}. This model leverages the sequential learning of weak classifiers to optimize classification performance over multiple iterations.

\subsection{Ensemble Learning (Voting Classifier)}

\begin{sloppypar}
	For further enhancement, we employed a soft voting ensemble that combines multiple classifiers, each with complementary strengths. The ensemble includes a \textbf{Gradient Boosting Classifier} (\texttt{n\_estimators=50}, \texttt{max\_depth=3}, \texttt{learning\_rate=0.1}), a \textbf{Random Forest Classifier} (\texttt{n\_estimators=50}, \texttt{max\_depth=3}), a \textbf{Support Vector Classifier (SVC)} with an RBF kernel and probability estimates enabled, and a \textbf{Bagging Classifier} (\texttt{n\_estimators=30}, \texttt{max\_samples=1.0}, \texttt{max\_features=1.0}). These classifiers were integrated using a \textbf{soft voting strategy} in the \texttt{VotingClassifier(voting='soft')} to average their probabilistic predictions.
	
	Feature reduction was again performed using PCA with 95\% variance retention. This ensemble approach leverages model diversity to achieve improved generalization and more stable predictions across sleep classes.
\end{sloppypar}


\section{Results and Evaluation With Machine Learning }

 


\subsection{Random Forest}

For the Random Forest model, we present the following evaluation metrics:

\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/random_forest_confusion_matrix.png}
		\caption{Random Forest Confusion Matrix}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/accuracy_curve.png}
		\caption{Random Forest Accuracy Curve}
	\end{minipage}
\end{figure}
	\vspace{1em}
\begin{figure}[H]
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/loss_curve.png}
		\caption{Random Forest Loss Curve}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/precision_per_class.png}
		\caption{Random Forest Precision Per Class}
	\end{minipage}
\end{figure}

The Random Forest classifier demonstrates solid performance with an accuracy of 0.764, precision of 0.777, and sensitivity of 0.770. Its F1-Score of 0.766 reflects a balanced performance between precision and recall. 

\subsection{Bagging Classifier}

For the Bagging Classifier model, we present the following evaluation metrics:

\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/BG_confusion_matrix.png}
		\caption{Bagging Classifier Confusion Matrix}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/accuracy_curve.png}
		\caption{Bagging Classifier Accuracy Curve}
	\end{minipage}
	
	\vspace{1em}
	
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/loss_curve.png}
		\caption{Bagging Classifier Loss Curve}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/precision_per_class.png}
		\caption{Bagging Classifier Precision Per Class}
	\end{minipage}
\end{figure}

The Gradient Boosting classifier achieves moderate performance with an accuracy of 0.702, precision of 0.687, and sensitivity of 0.689. Its F1-Score of 0.687 indicates a fairly balanced trade-off between precision and recall.


\subsection{Ensemble Learning}

For the Ensemble Learning model, we present the following evaluation metrics:

\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/Ensemble_confusion_matrix.png}
		\caption{Ensemble Learning Confusion Matrix}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/accuracy_curve.png}
		\caption{Ensemble Learning Accuracy Curve}
	\end{minipage}
	
	\vspace{1em}
	
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/loss_curve.png}
		\caption{Ensemble Learning Loss Curve}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/precision_per_class.png}
		\caption{Ensemble Learning Precision Per Class}
	\end{minipage}
\end{figure}

The Ensemble Learning model exhibits strong performance with an accuracy of 0.831, precision of 0.830, and sensitivity of 0.820. Its F1-Score of 0.819 underscores its effectiveness in maintaining a balance between precision and recall.



\subsection{Gradient Boosting}

For the Gradient Boosting model, we present the following evaluation metrics:

\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/confusion_matrix.png}
		\caption{Gradient Boosting Confusion Matrix}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/accuracy_curve.png}
		\caption{Gradient Boosting Accuracy Curve}
	\end{minipage}
	
	\vspace{1em}
	
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/loss_curve.png}
		\caption{Gradient Boosting Loss Curve}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/precision_per_class.png}
		\caption{Gradient Boosting Precision Per Class}
	\end{minipage}
\end{figure}


The Gradient Boosting Classifier shows competent performance with an accuracy of 0.753, precision of 0.737, and sensitivity of 0.746. Its F1-Score of 0.734 suggests a solid equilibrium between precision and recall.


\section{Results and Evaluation with K-fold Cross Validation}
\begin{table}[!h]
	\centering
	\caption{Classifier Performance Summary}
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Classifier} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Recall} & \textbf{F1-Score} \\
		\midrule
		Ensemble Learning & 0.95 & 0.86 & 0.79 & 0.82 \\
		Random Forest     & 0.88 & 0.85 & 0.81 & 0.82 \\
		Gradient Boosting & 0.87 & 0.83 & 0.81 & 0.82 \\
		SGD Classifier    & 0.80 & 0.73 & 0.70 & 0.70 \\
		\bottomrule
	\end{tabular}
\end{table}


\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/Ensemble Learning_percentage_based.png}
		\caption{Ensemble Learning Percentage-Based Confusion Matrix}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/Random Forest_percentage_based.png}
		\caption{Random Forest Percentage-Based Confusion Matrix}
	\end{minipage}
\end{figure}

\textbf{Analysis:} Ensemble Learning outperforms all other models with the highest average accuracy (95\%) and strong macro F1-score (0.82), particularly excelling in the Wake stage with near-perfect precision and recall. Random Forest and Gradient Boosting both deliver balanced performance with 88\% and 87\% accuracy respectively, showing robust results in N2 and REM stages. SGD Classifier, while computationally efficient, lags behind in performance, especially in accurately detecting the N1 stage, reflecting a much lower F1-score (0.70).





\begin{figure}[H]
	\centering
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/GB Classifier_percentage_based.png}
		\caption{Gradient Boosting Percentage-Based Confusion Matrix}
	\end{minipage}
	\hfill
	\begin{minipage}[t]{0.48\textwidth}
		\centering
		\includegraphics[width=\textwidth]{img/paper_1/SGD_percentage_based.png}
		\caption{SGD Percentage-Based Confusion Matrix}
	\end{minipage}
\end{figure}


































\section{Comparison Sections}





The comparison tables highlight that Gradient Boosting and Ensemble Learning consistently outperform other classifiers across overall and per-stage metrics, particularly in precision and F1-score.

\begin{figure}[H]
	\centering
	\includegraphics[width=0.99\linewidth]{img/paper_1/bar plot}
	\caption{Comparison of Classifier Performance Based on Precision, Recall, and F1-Score}
	\label{fig:bar-plot}
\end{figure}



 Notably, REM and Wake (W) stages achieve the highest precision and recall, while N1 and N2 stages remain more challenging for all models.
\begin{table}[h!]
	\centering
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Classifier} & \textbf{Accuracy} & \textbf{Precision} & \textbf{Sensitivity} & \textbf{F1-Score} \\
		\midrule
		Random Forest       & 0.764             & 0.777              & 0.770                & 0.766             \\
		Gradient Boosting   & 0.702             & 0.687              & 0.689                & 0.687             \\
		Ensemble Learning   & \textbf{0.831}    & \textbf{0.830}     & \textbf{0.820}       & \textbf{0.819}    \\
		Bagging Classifier  & 0.753             & 0.737              & 0.746                & 0.734             \\
		\bottomrule
	\end{tabular}
	\caption{ Performance Metrics of Classifiers}
\end{table}





\begin{table}[htbp]
	\centering
	
	\begin{tabular}{lcccc}
		\toprule
		\textbf{Stage} & \textbf{Metric} & \textbf{Random Forest} & \textbf{Ensemble (Gradient)} & \textbf{Bagging} \\
		\midrule
		\textbf{W}     & Precision       & 0.86                   & \textbf{0.97}                & 0.95             \\
		& Recall          & \textbf{0.91}          & 0.86                         & \textbf{0.97}    \\
		\textbf{N1}    & Precision       & 0.50                   & \textbf{0.57}                & 0.57             \\
		& Recall          & 0.59                   & \textbf{0.79}                & \textbf{0.79}    \\
		\textbf{N2}    & Precision       & 0.48                   & \textbf{0.68}                & 0.56             \\
		& Recall          & 0.44                   & \textbf{0.56}                & 0.41             \\
		\textbf{REM}   & Precision       & 0.93                   & \textbf{0.98}                & 0.87             \\
		& Recall          & 0.88                   & \textbf{0.97}                & 0.93             \\
		\bottomrule
	\end{tabular}

\end{table}




