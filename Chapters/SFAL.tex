We introduce a novel federated framework, as depicted in Figure \ref{Fig: fe4dFramework}, which incorporates a straightforward yet potent strategy utilizing an adaptive scale-aware weighted aggregation (ASWA) technique. This framework not only integrates local client training but also implements our suggested spatial-feature alignment loss (SFAL). Particularly in scenarios involving non-IID brain tumor data, a discernible shift occurs during local training, resulting in the global model consistently outperforming the local model in representation. Our primary aim revolves around minimizing the disparity between the representation acquired by the local model and that attained by the global model. Concurrently, we aim to maximize the difference between the representation learned by the local model and the one obtained by its previous iteration, drawing inspiration from contrastive learning, a widely utilized technique in visual representation development. The entire workflow of our proposed method is visually illustrated in Figure \ref{Fig: fe4dFramework}. In subsequent sections, we will delve deeper into the intricacies of the network architecture, provide detailed explanations of the local learning objective, and offer insights into the enhanced aggregation process.

\begin{figure}[!h]
  \centering
   \includegraphics[width=\linewidth]{Images_4th/ColorFlowchat.pdf}
    \caption{Overview of the new federated framework, presenting two strategies: (i) SFAL (Modified Dice + Enhanced Contrastive), aiming to achieve a balance between spatial precision and feature similarity in images. This involves combining modified dice loss and our improved contrastive loss at the model level during local training of individual clients. And (ii) ASWA, ensuring a balanced influence in the aggregation process at the global model from the scales of local client data.}
  \label{Fig: fe4dFramework}
\end{figure}

\section{Architectural Details}
Our segmentation strategy, depicted visually in Figure \ref{Fig: fe4dFramework}, employs the 3D U-Net architecture \cite{cai2022novel,11} as our preferred model for segmentation. The efficacy of the 3D U-Net stems from its adeptness at leveraging 3D contextual cues, facilitating the segmentation of volumetric data with notable precision. To encapsulate the essence of the inputs, we extract representation vectors from the final convolution layer of the 3D U-Net encoder. For the sake of clarity, we designate the model weight as $w$, and the mapped representation vector of input $x$ as $U_w$. Local loss function consists of two fundamental elements. The first component, our adapted dice loss ($L_{Dice}$), serves as a foundational aspect for our local training. The second component, our proposed enhanced contrastive loss function ($L_{EC}$), is formulated to enhance the discriminative capability of our segmentation model.

In the context of client engagement in the local training process, let's consider a specific client $c_i$. Initially, the client obtains the global model ($w^t$) from the server. Subsequently, during the local training phase, client $c_i$ updates its model to $w^t_i$. For each input $x$, we derive its representation from three sources: the global model $w^t$ (represented as $z_g = U_{w^t}(x)$), the local model from the previous round $w^{(t-1)}_i$ (represented as $z_p = U_{w^{(t-1)}_i}(x)$), and the current local model being updated $w^{t}_i$ (represented as $z_c = U_{w^{t}_i}(x)$). Given the comprehensive representation power of the global model across all data, our objective is to minimize the distance between $z_c$ and $z_g$, while simultaneously maximizing the distance between $z_c$ and $z_p$. This approach is designed to align the local model's representation with the global model, thereby capturing the overarching contextual information. At the same time, it aims to differentiate the local model's representation from that of the previous round to ensure progressive learning throughout the federated training process.

\section{Local Spatial-Feature Alignment Loss (SFAL) Function}
In the methodology we propose, we introduce a specialized loss function referred to as the spatial-feature alignment loss (SFAL). This distinctive loss function is designed to merge two crucial elements: the modified dice loss and an upgraded contrastive loss. The primary objective is to comprehensively encapsulate both spatial and feature-related information, thereby ensuring a robust segmentation process. The incorporation of the modified dice loss plays a pivotal role in achieving precise spatial localization, emphasizing the significance of accurate segmentation, which contributes to the overall spatial coherence of the segmented regions. Simultaneously, the inclusion of our enhanced contrastive loss aims to enhance the quality of feature representation, making the segmentation process more discriminative and facilitating a deeper understanding of the underlying features. To offer a mechanism for flexible adjustment, we introduce a hyperparameter denoted as \(\omega\) within the SFAL. This hyperparameter functions as a fine-tuning tool, enabling a balanced trade-off between prioritizing precise segmentation guided by the modified dice loss and enhancing discriminative feature learning influenced by the enhanced contrastive loss. This adaptability allows for a refined optimization process, where our local spatial-feature alignment loss function (\(L_{SFAL}\)) optimally harnesses the strengths of both loss components. The significance of this adaptability is underscored by the improved generalization observed in the outcomes of our experiments. The equilibrium achieved through the combination of these components is precisely articulated in Equation \ref{Eq: SFAL}, providing a mathematical representation of the balanced integration of spatial and feature considerations within our segmentation framework. This refined optimization process highlights the effectiveness of our SFAL function in capturing the intricacies of spatial and feature relationships, ultimately leading to superior segmentation outcomes.

\begin{equation} \label{Eq: SFAL}
    L_{SFAL} = \omega \times L_{Dice} + (1 - \omega) \times L_{EC}
\end{equation}

Here, the parameter $\omega$ serves as a hyperparameter governing the contribution of the dice loss to the SFAL loss, while the term \((1 - \omega)\) regulates the influence of the enhanced contrastive loss. It is essential to note that the sum of $\omega$ and \((1-\omega)\) equals 1.

\subsection{Modified Dice Loss}
The modified dice loss function, which has been tailored to suit our specific requirements, serves as a metric for assessing the degree of overlap between two binary vectors on a pixel-by-pixel basis. This assessment enables us to discern the disparities between the binary masks generated by our model and the ground truth across all dimensions of an image. Mathematically, the modified dice loss can be denoted as follows:

\begin{equation} \label{Eq: Dice Loss}
\text{Loss}_{\text{Dice}} = 1 - \frac{2 \sum_{i,j,k} y_{ijk} \cdot \hat{y}_{ijk} + \epsilon}{\sum_{i,j,k} y_{ijk} + \sum_{i,j,k} \hat{y}_{ijk} + \epsilon}
\end{equation}

Here, \(y_{ijk}\) represents the binary mask derived from the ground truth, while \(\hat{y}_{ijk}\) indicates the corresponding prediction generated by our model. The numerator of this expression computes the double intersection of the masks, incorporating a smoothing term \(\epsilon\) to ensure stability. The denominator calculates the sum of the individual areas for both the ground truth and predicted masks, again integrating the smoothing term \(\epsilon\) to prevent division by zero. This formulation guarantees that the modified dice loss effectively captures the distinctions between the predicted and actual binary masks, thereby serving as a robust evaluation metric for our segmentation model.

\subsection{Enhanced Contrastive Loss Function}

The enhanced contrastive loss function has been meticulously crafted with the specific objective of reducing the disparity between the representation acquired through the local model and that attained from the global model. In addition, it concurrently strives to widen the discrepancy between the representation learned by the local model and its previous iteration. To delve deeper into the complexities of this loss function, it is imperative to examine the multidimensional representation vectors originating from various sources, namely the global model (\(z_g\)), the current local model (\(z_c\)), and the preceding local model (\(z_p\)). These vectors are characterized by five distinct indices: \(n, i, j, k, l\), each representing different dimensions within the embedding space. Specifically, \(n\) denotes the batch size, while \(i, j, k\) delineate the spatial dimensions of the image, and \(l\) signifies the feature index. To ensure uniformity and facilitate unit length adjustment, a normalization procedure is employed on these vectors, whereby they are scaled by the reciprocal of their L2 norm across spatial dimensions. This normalization process is mathematically articulated through the following equations:

\begin{equation}
    z_{g_{ijk}} = \frac{z_{g_{ijk}}}{\sqrt{\sum_{i,j,k} (z_{g_{ijk}})^2}},
\end{equation}

\begin{equation}
    z_{c_{ijk}} = \frac{z_{c_{ijk}}}{\sqrt{\sum_{i,j,k} (z_{c_{ijk}})^2}},
\end{equation}

\begin{equation}
    z_{p_{ijk}} = \frac{z_{p_{ijk}}}{\sqrt{\sum_{i,j,k} (z_{p_{ijk}})^2}}.
\end{equation}

The process of normalization serves the crucial purpose of ensuring that the vectors are positioned precisely on the unit hypersphere, thereby simplifying the examination of directional relationships, which holds paramount importance in the realm of similarity calculations. Through this normalization procedure, a uniform scale is established, thereby providing a consistent foundation for gauging similarity. In our pursuit of attaining a thorough comprehension of contextual comparisons, we undertake a meticulous examination of the relationships inherent in feature representations. Our initial focus revolves around assessing the degree of similarity between the representation of the current local model (\(z_c\)) and that of the server model (\(z_g\)), a relationship articulated by:

\begin{equation} \label{eq: sim_zc_zg}
     \text{sim}(z_c, z_g) = \sum_{i,j,k} z_{c_{ijk}} \times z_{g_{ijk}}.
\end{equation}

This comparative analysis furnishes invaluable insights into the extent to which the feature vectors of the current local model correspond with those emanating from the global server model. The normalization to the unit hypersphere guarantees that these representations are standardized, thereby facilitating a meaningful assessment of their alignment. Furthermore, the evaluation of similarity between the representation of the current local model (\(z_c\)) and that of the preceding local model (\(z_p\)) is expressed by:

\begin{equation} \label{eq: sim_zc_zp}
     \text{sim}(z_c, z_p) = \sum_{i,j,k} z_{c_{ijk}} \times z_{p_{ijk}}.
\end{equation}

This examination offers detailed insights into the evolution and consistency of the feature vectors within the local model across successive time points. By employing a technique that involves multiplying individual components normalized across various spatial dimensions \(i\), \(j\), and \(k\), we delve into a granular analysis of the directional alignment and sensitivity to magnitude within these representations. Such an analysis holds particular significance in medical image segmentation tasks, where the interrelationships among these vectors significantly impact the precision with which the model delineates and separates distinct regions within an image. Instead of resorting to the conventional method of calculating dot products, we have chosen to utilize element-wise multiplication between the vectors representing the current local model (\(z_c\)) and both the server model (\(z_g\)) and the previous local model (\(z_p\))). This choice is motivated by the element-wise multiplication's ability to highlight specific elements where both feature vectors exhibit non-zero values. Essentially, this operation functions akin to a filter, amplifying elements where patterns align (thus emphasizing corresponding features) and attenuating elements where patterns diverge. The construction of the enhanced contrastive loss function (\(L_{EC}\)) is tailored to capitalize on the similarities between representations, with a specific emphasis on \(z_c\), \(z_p\), and \(z_g\). By quantifying the alignment and disparities among these representations, \(L_{EC}\) encourages the model to adapt to the overarching server model while preserving consistency with its prior states. The introduction of a temperature parameter (\(\tau\)) within the exponentials modulates the steepness of the probability distributions. A higher temperature facilitates a softer distribution, fostering a more nuanced exploration of model representations, whereas a lower temperature accentuates differences. Taking the logarithm of the ratio of exponentials and subsequently applying the negative function, as commonly observed in contrastive losses, effectively penalizes dissimilarities, prompting the model to prioritize similarities during its learning process. Furthermore, normalizing the loss by dividing it by \(N\), representing the total number of batch samples, ensures an average loss per sample, thus facilitating a coherent evaluation of model performance. The overarching equation for the enhanced contrastive loss function (\(L_{EC}\)) is thus expressed as:

\begin{equation} \label{eq: Enhanced Contrastive loss}
    L_{EC} = -\frac{1}{N} \sum_{i=1}^{N} \left[ \log\left( \frac{e^{\frac{\text{sim}(z_c, z_g)}{\tau}}}{e^{\frac{\text{sim}(z_c, z_g)}{\tau}} + e^{\frac{\text{sim}(z_c, z_p)}{\tau}}} \right) \right] \times \text{lr}.
\end{equation}

This formulation encapsulates the essence of the enhanced contrastive loss function, elucidating its pivotal role in steering the model's learning trajectory by accentuating adaptation to global representations while preserving consistency with historical states. The incorporation of the temperature parameter \(\tau\) and the learning rate \(\text{lr}\) endows this process with flexibility and control, thereby further augmenting the model's adaptability and convergence.

\section{Adaptive Scale-aware Weighted Aggregation (ASWA)}
In our proposed framework for federated learning, the adaptive scale-aware weighted aggregation (ASWA) method plays a crucial role in updating the server model \(w^{t+1}\) across all participating clients. This particular approach, which is executed centrally at the server, takes careful account of the varying sizes of local datasets (\(n_i\)) present on each client. The primary objective is to facilitate a nuanced and flexible integration of information from diverse sources. The determination of weights assigned to each client's contribution (\(w_i^t\)) is carried out meticulously by considering the ratio of its dataset size (\(n_i\)) to the total dataset size across all clients (\(N_{total}\)). This process can be succinctly represented mathematically as:

\begin{equation} \label{Eq: WEight update Equation}
    w^{t+1} = \sum_{i=1}^N \frac{n_i}{N_{total}} w_i^t.
\end{equation}

The essence of this adaptive scaling mechanism lies in its recognition that clients with larger datasets should exert a more significant influence on the global model update. By adopting this approach, a balanced assimilation of information is fostered, acknowledging the inherent diversity within local datasets. ASWA thus enhances the federated learning paradigm by dynamically adjusting the impact of individual clients based on the scale of their data holdings. This ensures the development of a global model that is not only robust but also representative, capturing the intricacies inherent in various local datasets.
