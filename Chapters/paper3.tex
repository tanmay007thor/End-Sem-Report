
\section{Methodology}



The SleepGCN-Transformer is a hybrid deep learning framework designed for accurate sleep stage classification by combining the strengths of Graph Convolutional Networks (GCNs) and Transformer architectures. This model is trained on the Sleep-EDF dataset, utilizing four critical physiological signal channels: EEG Fpz-Cz, EEG Pz-Oz, EOG (horizontal), and submental EMG.


\begin{figure}
	\centering
	\includegraphics[width=0.8\linewidth]{"img/paper_3/hypnogram sleep stage transition"}
	\caption{Hypnogram With Transition Heighlights }
	\label{fig:hypnogram-sleep-stage-transition}
\end{figure}




In this architecture, GCNs are employed to capture spatial relationships between the multimodal input signals, while the Transformer encoder is tasked with modeling the temporal dependencies across time sequences. This dual approach ensures that both spatial and sequential dynamics are effectively learned.

Experimental results confirm the effectiveness of the model, achieving an accuracy of 93.12\% on the training set and 93.04\% on the validation set. To address class imbalance—particularly the underrepresentation of certain sleep stages—the model incorporates Focal Loss, which emphasizes harder-to-classify samples during training.

Further analysis of feature importance revealed that the EMG and EEG Pz-Oz channels play a significant role in predicting sleep stages. This methodological framework not only enhances classification performance but also supports the integration of explainable AI techniques, contributing to the development of interpretable tools for clinical sleep analysis and decision-making.

\begin{figure}
	\centering
	\includegraphics[width=.9\linewidth]{"img/paper_3/Graph Convolution Neural Network -SleepGCN_Transformer Architechture"}
	\caption{Proposed Architecture of the sleep GCN- Transformer Architecture}
	\label{fig:graph-convolution-neural-network--sleepgcntransformer-architechture}
\end{figure}



\section{Dataset and Implementation Environment}

Our experiments were conducted using the publicly available Sleep-EDF dataset, which is widely utilized for sleep stage classification research. The dataset can be accessed at \url{https://physionet.org/content/sleep-edfx/1.0.0/}, and it includes EEG recordings labeled with corresponding sleep stages, enabling effective supervised learning.

The implementation was carried out in Python 3.13.1 using the PyTorch deep learning framework. Development was performed in Visual Studio Code (VS Code) on a machine powered by the Apple M1 chip, utilizing both CPU and Metal acceleration capabilities for training efficiency.

For version control and collaborative development, we used GitHub to manage our codebase. The repository containing our source code is publicly available at \url{https://github.com/tanmay007thor/GCN}.

The source code of this research is licensed under the Apache License 2.0, which allows for open use and distribution. The license details are available at \url{http://www.apache.org/licenses/}.

To support data processing, model training, and evaluation, we relied on several key Python libraries. The primary libraries and their versions used in our project include: NumPy 2.2.2, Pandas 2.2.3, Scikit-learn 1.6.1, SciPy 1.15.1, Matplotlib 3.10.0, Seaborn 0.13.2, PyTorch 2.6.0, Torch Geometric 2.6.1, NetworkX 3.4.2, and MNE 1.9.0. This software environment ensured a reliable, efficient, and reproducible workflow throughout our research.



\section{Preprocessing Techniques}
\begin{figure}[H]
	\centering
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/paper_3/Signals}
		\caption{Raw signal channels from the Sleep-EDF dataset}
		\label{fig:raw_signals}
	\end{minipage}
	\hfill
	\begin{minipage}{0.48\textwidth}
		\centering
		\includegraphics[width=\linewidth]{img/paper_3/filtered chanels}
		\caption{Preprocessed and filtered signal channels}
		\label{fig:filtered_signals}
	\end{minipage}
\end{figure}


\subsection{Data Information}

The Sleep-EDF dataset comprises multichannel physiological signals, including EEG, EOG, EMG, nasal airflow, and temperature, each accompanied by event markers. These signals are recorded over 60-second intervals, with frequency content varying between 60 and 120 Hz. The dynamic nature of these signals introduces significant variability, making it challenging to identify consistent patterns for classification. Such rapid fluctuations can increase the likelihood of misclassification. Furthermore, the dataset presents several challenges such as high-frequency noise, class imbalance (notably a dominance of the Wake stage), and a substantial number of time steps—approximately 100 time steps per second—adding complexity to the task of modeling sleep stages.

\subsection{Preprocessing Pipeline}

To address these challenges, we developed a comprehensive preprocessing pipeline focused on selecting the most informative channels and reducing irrelevant data. Initially, we discarded several channels that were found to contribute minimally to sleep stage classification. The retained signals included the EEG channels (Fpz-Cz and Pz-Oz), the horizontal EOG channel (to track eye movement), and the submental EMG channel (for monitoring chin muscle activity).

Each sleep stage in the dataset was mapped to a simplified class label to consolidate similar stages and reduce classification ambiguity. Table~\ref{tab:sleep_stage_mapping} presents the sleep stage-to-class mapping:

\begin{table}[H]
	\centering
	\caption{Sleep Stage Mapping}
	\label{tab:sleep_stage_mapping}
	\begin{tabular}{llc}
		\hline
		\textbf{Sleep Stage} & \textbf{Label} & \textbf{Mapped Class} \\
		\hline
		Wake                & 0 & 0 \\
		Stage 1             & 1 & 1 \\
		Stage 2             & 2 & 2 \\
		Stage 3             & 3 & 3 \\
		Stage 4             & 4 & 3 \\
		REM                 & 5 & 4 \\
		\hline
	\end{tabular}
\end{table}

The preprocessing begins with reading the EDF recordings and assigning sleep labels based on the mapped classes. Subsequently, we applied a bandpass filter to isolate relevant frequency ranges. Specifically, the signal was filtered to retain only the 0.3–30 Hz band using the firwin filter design, effectively removing noise and irrelevant frequencies outside this band. Though not a standard approach, this customized filtering was found to be better suited to our specific dataset and classification task.

We then segmented the signals into fixed-length epochs of 30 seconds. The final preprocessed data was shaped into a three-dimensional format: $(X, 4, 3000)$, where $X$ is the number of epochs (samples), $4$ is the number of selected channels, and $3000$ corresponds to the number of time steps per epoch.

\subsection{Filtering Method}

The filtering was implemented using a windowed time-domain design via the firwin method. A Hamming window was employed, configured with a 0.0194 passband ripple and 53 dB stopband attenuation. The filter characteristics were as follows: lower passband edge at 0.30 Hz, lower transition bandwidth of 0.30 Hz (with a $-6$ dB cutoff at 0.15 Hz), upper passband edge at 30.00 Hz, and an upper transition bandwidth of 7.50 Hz (with a $-6$ dB cutoff at 33.75 Hz). These settings were carefully chosen to preserve the signal components relevant to sleep staging while minimizing noise.







\begin{algorithm}[H]
	\caption{Main Pipeline for Sleep Stage Classification}
	\label{alg:main}
	\textbf{Input:} list of (PSG file, Hypnogram file) pairs\\
	\textbf{Output:} trained SleepGCN\_Transformer model
	\begin{enumerate}
		\item fnames $\leftarrow$ [('SC4001E0-PSG.edf','SC4001EC-Hypnogram.edf')]\label{step:fnames}
		\item $(X_{\mathrm{all}},Y_{\mathrm{all}}) \leftarrow \texttt{preprocess\_data}(fnames)$
		\item $X_{\mathrm{tensor}} \leftarrow \texttt{to\_tensor}(X_{\mathrm{all}})$; \quad
		$Y_{\mathrm{tensor}} \leftarrow \texttt{to\_tensor}(Y_{\mathrm{all}})$
		\item dataset $\leftarrow$ \texttt{SleepEEGDataset}($X_{\mathrm{tensor}},Y_{\mathrm{tensor}}$)
		\item train\_size $\leftarrow 0.8\times |\text{dataset}|$; \quad
		val\_size $\leftarrow |\text{dataset}| - \text{train\_size}$
		\item $(\text{train\_ds},\text{val\_ds}) \leftarrow \texttt{split}(dataset,\,[\text{train\_size},\text{val\_size}])$
		\item model $\leftarrow \texttt{init\_model}(4,256,5,5,4,0.1)$
		\item trainer $\leftarrow \texttt{init\_trainer}(model,\text{train\_ds},\text{val\_ds},32,20,1e-4)$
		\item \texttt{trainer.train()}
	\end{enumerate}
\end{algorithm}

\begin{algorithm}[H]
	\caption{SleepEEGDataset Class Definition}
	\label{alg:sleepdataset}
	\textbf{Input:} feature matrix $X$, labels $Y$\\
	\textbf{Output:} graph dataset with node features and edges
	\begin{enumerate}
		\item \textbf{class} SleepEEGDataset
		\begin{enumerate}
			\item \_\_init\_\_(self,$X,Y$):
			\begin{enumerate}
				\item self.X$\leftarrow X$; self.Y$\leftarrow Y$
				\item $A\leftarrow\texttt{build\_adjacency}(X)$
				\item self.edge\_index$\leftarrow \texttt{find\_edges}(A)$
				\item self.edge\_weights$\leftarrow \texttt{edge\_weights}(A)$
			\end{enumerate}
			\item len(self): \textbf{return} $|\,$self.X$|$
			\item get(self,idx):
			\begin{enumerate}
				\item node\_feat$\leftarrow$ self.X[idx]
				\item label$\leftarrow$ self.Y[idx]
				\item \textbf{return} (node\_feat,self.edge\_index,self.edge\_weights,label)
			\end{enumerate}
		\end{enumerate}
	\end{enumerate}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Preprocess Sleep EEG Data}
	\label{alg:preprocess_data}
	\textbf{Input:} list of (psg\_file, ann\_file)\\
	\textbf{Output:} feature matrix $X_{\mathrm{all}}$, labels $Y_{\mathrm{all}}$
	\begin{enumerate}
		\item $X_{\mathrm{all}}\leftarrow\emptyset$, $Y_{\mathrm{all}}\leftarrow\emptyset$
		\item \textbf{for each} $(p,a)$ in fnames:
		\begin{enumerate}
			\item raw$\leftarrow$ load PSG from $p$
			\item annots$\leftarrow$ load annotations from $a$
			\item raw$\leftarrow$ filter and preprocess(raw)
			\item $E\leftarrow$ extract epochs(raw,annots)
			\item $L\leftarrow$ extract labels(annots)
			\item $X_{\mathrm{all}}\leftarrow X_{\mathrm{all}}\cup E$
			\item $Y_{\mathrm{all}}\leftarrow Y_{\mathrm{all}}\cup L$
		\end{enumerate}
		\item \textbf{return} $X_{\mathrm{all}},Y_{\mathrm{all}}$
	\end{enumerate}
\end{algorithm}

\begin{algorithm}[H]
	\caption{Training Pipeline for Sleep EEG Model}
	\label{alg:training_pipeline}
	\textbf{Input:} training set, validation set, model, epochs, batch size \\
	\textbf{Output:} trained model, performance metrics
	\begin{enumerate}
		\item loss\_fn$\leftarrow$ define loss()
		\item optimizer$\leftarrow$ init optimizer(model)
		\item scheduler$\leftarrow$ init scheduler(optimizer,epochs)
		\item \textbf{for} epoch = 1 to epochs:
		\begin{enumerate}
			\item model.train()
			\item \textbf{for each} batch in train\_ds:
			\begin{enumerate}
				\item output$\leftarrow$model(batch)
				\item loss$\leftarrow$loss\_fn(output, batch.y)
				\item backpropagate and step optimizer
			\end{enumerate}
			\item scheduler.step()
			\item model.eval()
			\item \textbf{for each} batch in val\_ds:
			\begin{enumerate}
				\item output$\leftarrow$model(batch)
				\item compute validation metrics
			\end{enumerate}
		\end{enumerate}
		\item \textbf{return} trained model, metrics
	\end{enumerate}
\end{algorithm}

\begin{algorithm}[H]
	\caption{SleepGCN\_Transformer Model Declaration}
	\label{alg:sleep_gcn_transformer}
	\textbf{Input:} input\_dim, hidden\_dim, output\_dim, num\_layers, nhead, dropout \\
	\textbf{Output:} network layers initialized
	\begin{enumerate}
		\item Initialize list of GCN layers and batch norms
		\item \textbf{for} $i$ in $1\ldots$num\_layers:
		\begin{enumerate}
			\item append GCNConv layer
			\item append BatchNorm layer
		\end{enumerate}
		\item Initialize Transformer encoder with nhead and dropout
		\item Initialize final fully‐connected layer
		\item \textbf{return} model components
	\end{enumerate}
\end{algorithm}








\section{Model Architecture and Learning Framework}

To extract spatial features from the multi-channel data, we construct a graph using the four selected channels, where each channel acts as a node and the edges represent their interrelationships. These edges are weighted based on prior literature and empirical observations, assigning higher importance to EEG channels due to their strong influence on sleep stage detection. In contrast, connections involving EMG and EOG are assigned lower weights, as EOG shows weaker correlations. Although assigning self-loop weights is possible, we chose not to include them in this implementation.


\begin{figure}
	\centering
	\includegraphics[width=.9\linewidth]{"img/paper_3/Graph Convolution Neural Network -SleepGCN_Transformer Architechture"}
	\caption{Proposed Architecture of the sleep GCN- Transformer Architecture}
	\label{fig:graph-convolution-neural-network--sleepgcntransformer-architechture}
\end{figure}

The final dataset comprises 11,076 samples, structured as \texttt{Data(x=[3000, 4], edge\_index=[2, 12], edge\_attr=[12], y=[1])}. Here, \texttt{x} represents the time-series data with 3,000 time steps across 4 channels, \texttt{edge\_index} defines the graph connectivity, \texttt{edge\_attr} contains the edge weights, and \texttt{y} denotes the sleep stage label. This dataset is created using the \texttt{SleepEEGDataset} class and is designed to effectively model spatial dependencies across EEG, EMG, and EOG signals, thereby enhancing sleep stage classification performance.


\begin{figure}
	\centering
	\includegraphics[width=0.7\linewidth]{"img/paper_3/Graph Weightage"}
	\caption{Graph Dataset Creation}
	\label{fig:graph-weightage}
\end{figure}

\subsection{GCN Architecture}

To capture spatial dependencies across the selected EEG, EMG, and EOG channels, we incorporate a Graph Convolutional Network (GCN) into our model. Each sample is structured as a graph with four nodes—corresponding to the four selected channels—and each node contains a time series of 3,000 time steps. The dataset comprises 11,076 such graph-based samples.

The GCN consists of five identical graph convolution layers. The initial input has a shape of (4, 3000), which is projected to (4, 256) after the first graph convolution. Each layer includes batch normalization (256 units), followed by a ReLU activation function and a dropout of 0.1. After the final GCN layer, we apply global mean pooling to obtain a graph-level representation.

The propagation rule used in each GCN layer is given by:

\begin{equation}
	H^{(l+1)} = \sigma\left(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} H^{(l)} W^{(l)}\right)
\end{equation}

\begin{itemize}
	\item $H^{(l)} \in \mathbb{R}^{N \times D}$ is the feature matrix at layer $l$, where $N$ is the number of nodes and $D$ is the feature dimension.
	\item $\tilde{A} = A + I$ is the adjacency matrix with self-loops.
	\item $\tilde{D}$ is the degree matrix corresponding to $\tilde{A}$.
	\item $W^{(l)}$ is the learnable weight matrix for layer $l$.
	\item $\sigma$ denotes a non-linear activation function, typically ReLU.
\end{itemize}

The simplified form for one GCN layer operation is:

\begin{equation}
	X' = \sigma\left(\tilde{D}^{-1/2} \tilde{A} \tilde{D}^{-1/2} X W\right)
\end{equation}

To aggregate node features into a graph-level embedding, we use global mean pooling:

\begin{equation}
	x = \frac{1}{|V|} \sum_{v \in V} h_v
\end{equation}

\begin{itemize}
	\item $V$ is the set of nodes in the graph.
	\item $h_v$ is the feature vector of node $v$.
	\item $x$ is the pooled feature vector representing the entire graph.
\end{itemize}

We apply standard activation and normalization functions during training:

\begin{equation}
	\text{ReLU}(x) = \max(0, x)
\end{equation}

\begin{equation}
	\text{softmax}(x_i) = \frac{e^{x_i}}{\sum_j e^{x_j}}
\end{equation}

\begin{equation}
	x'' = \frac{x' - \mu}{\sigma}
\end{equation}

\begin{itemize}
	\item $\mu$ is the mean of $x'$.
	\item $\sigma$ is the standard deviation of $x'$.
	\item $x''$ is the normalized feature representation.
\end{itemize}


\subsection{Transformer Architecture}

To capture temporal dependencies in the input signal, we utilize a Transformer encoder comprising two stacked layers. Initially, the input is projected into a continuous vector space using an embedding layer. Since Transformers lack inherent mechanisms for handling sequential order, positional encodings are added to preserve time-step information. 

Following embedding and encoding, we use multi-head self-attention to allow the model to focus on different aspects of the signal simultaneously. The attention outputs are concatenated and passed through a feedforward network with dropout regularization. Finally, a fully connected classifier outputs logits for four-class sleep stage classification.

The operations within the Transformer encoder are summarized using the following equations:
\subsection{Transformer: Key Components and Equations}

\begin{itemize}
	\item \textbf{Input Embedding:} Maps input tokens into a continuous vector space.
	\begin{equation}
		X_{\text{embed}} = X W_{\text{embed}}
	\end{equation}
	
	\item \textbf{Positional Encoding:} Adds sequence order information using sine and cosine functions.
	\begin{equation}
		PE(\text{pos}, 2i) = \sin\left(\frac{\text{pos}}{10000^{2i/d}}\right)
	\end{equation}
	\begin{equation}
		PE(\text{pos}, 2i+1) = \cos\left(\frac{\text{pos}}{10000^{2i/d}}\right)
	\end{equation}
	
	\item \textbf{Query, Key, and Value Computation:} Core of attention mechanism, derived from input.
	\begin{equation}
		Q = XW_q,\quad K = XW_k,\quad V = XW_v
	\end{equation}
	
	\item \textbf{Scaled Dot-Product Attention:} Computes weighted output of values using attention scores.
	\begin{equation}
		H_1 = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right) V
	\end{equation}
	
	\item \textbf{Multi-Head Attention:} Combines multiple attention outputs for diverse feature learning.
	\begin{equation}
		H = \text{Concat}(H_1, H_2, ..., H_h) W_0
	\end{equation}
\end{itemize}









































\subsection{Training Pipeline}

\textbf{Fully Connected Layer:}  
The final stage of the model includes a linear transformation layer that maps a 256-dimensional input vector to 5 output logits, corresponding to the five target sleep stages. This transformation enables direct classification output.

\subsubsection*{Why Focal Loss Instead of Standard Cross-Entropy?}

Standard cross-entropy loss treats all training samples equally, which can lead to a bias toward majority classes in imbalanced datasets. In such settings, the model tends to suppress the prediction of minority classes, resulting in poor generalization. Focal Loss addresses this by dynamically adjusting the contribution of each sample based on the confidence of its prediction. This mechanism reduces the influence of well-classified samples and increases the focus on harder, misclassified instances.

Focal Loss introduces two key parameters: the focusing parameter $\gamma$, which controls the down-weighting of easy examples, and the class weighting factor $\alpha$, which helps balance class frequencies during training. These features make Focal Loss particularly effective for classification tasks involving significant class imbalance.

\subsubsection*{Focal Loss Formulation}

The Focal Loss is mathematically expressed as:

\[
FL(p_t) = -\alpha(1 - p_t)^{\gamma} \log(p_t)
\]

Here, $p_t$ denotes the predicted probability for the target class, $\alpha$ is the balancing factor to compensate for class imbalance, and $\gamma$ adjusts the focus on misclassified examples.

To enhance numerical stability and improve generalization, label smoothing is applied:

\[
y_{\text{smooth}} = y(1 - \epsilon) + \frac{\epsilon}{C}
\]

This prevents issues such as $\log(0)$ and ensures a more stable optimization process. The combined implementation of Focal Loss with label smoothing can be written in a PyTorch-like style as:

\[
L = \alpha(1 - p)^{\gamma} \left( - y_{\text{smooth}} \log(p) \right)
\]

\subsubsection*{Learning Rate Scheduling}

The learning rate is a critical hyperparameter that governs the pace of model updates during training. A high learning rate may cause divergence, while a low learning rate can result in slow convergence or suboptimal local minima. Therefore, adaptive learning rate scheduling is essential for efficient training.

CosineAnnealingLR is used as the scheduling strategy, as it provides a smooth and gradual reduction in the learning rate. It allows for large updates in the early stages of training and fine-tuning in later stages, thereby helping the model generalize better. The learning rate at each epoch $t$ is computed using the following cosine annealing formula:

\[
\eta_t = \eta_{\min} + \frac{1}{2} (\eta_{\max} - \eta_{\min}) \left( 1 + \cos \left( \frac{\pi T_{\text{cur}}}{T_{\text{max}}} \right) \right)
\]

In this equation, $\eta_t$ is the learning rate at epoch $t$, $\eta_{\min}$ and $\eta_{\max}$ are the minimum and maximum learning rates respectively, $T_{\text{cur}}$ is the current epoch, and $T_{\text{max}}$ is the total number of epochs.

\subsubsection*{Training Methodology Overview}

The training pipeline is encapsulated within a dedicated class named \texttt{SleepTrainer}, which manages model training, validation, and optimization. The training process begins with the computation of class weights to address data imbalance. The model is then trained using mini-batch gradient descent, where each batch is used to compute the loss and update the model parameters accordingly. Performance is periodically evaluated on a separate validation set, and the learning rate is adjusted dynamically using the scheduler.

\subsubsection*{Hyperparameters Used}

The training setup uses a batch size of 32 and an initial learning rate of 0.0003. The optimizer of choice is AdamW, with a weight decay of $1 \times 10^{-4}$ to prevent overfitting. Training is conducted for 20 epochs, and the CosineAnnealingLR scheduler is applied to adjust the learning rate over time. This configuration ensures smooth convergence and robust generalization.

\subsubsection*{Handling Class Imbalance}

EEG sleep stage data is inherently imbalanced, with some stages appearing far more frequently than others. To address this, class weights are computed and incorporated into the loss function. The weighting formula is given by:

\[
w_c = \left( \frac{\text{Total Samples}}{\text{Class Count} + 1} \right)^{0.5}
\]

This formulation ensures that less frequent classes receive proportionally higher weights, encouraging the model to give them more attention during training. These computed weights are then integrated directly into the Focal Loss function to balance the influence of each class.

\subsubsection*{Testing Data Distribution Analysis}

Balanced testing data is crucial for an unbiased evaluation of model performance. An imbalanced test set can lead to skewed metrics that do not accurately reflect the model’s generalization capabilities. In this pipeline, the testing data is curated to ensure an even distribution across all sleep stages.  













































\section{Results and Evaluation}

\begin{center}
	\begin{minipage}[t]{0.48\textwidth}
		\begin{align}
			\text{Accuracy} &= \frac{TP + TN}{TP + TN + FP + FN} \\
			\text{Precision} &= \frac{TP}{TP + FP} \\
			\text{Recall} &= \frac{TP}{TP + FN} \\
			\text{F1\text{-}Score} &= 2 \cdot \frac{\text{Precision} \cdot \text{Recall}}{\text{Precision} + \text{Recall}} \\
			\text{Support} &= TP + FN \\
			\text{Specificity} &= \frac{TN}{TN + FP}
		\end{align}
	\end{minipage}
	\hfill	
	\begin{minipage}[t]{0.48\textwidth}
		\begin{align}
			\text{Macro Precision} &= \frac{1}{C} \sum_{i=1}^{C} \frac{TP_i}{TP_i + FP_i} \\
			\text{Weighted Precision} &= \sum_{i=1}^{C} \frac{n_i}{N} \cdot \frac{TP_i}{TP_i + FP_i} \\
			\mathcal{L}{CE} &= -\sum{i=1}^{C} y_i \log(\hat{y}_i) \\
			\mathcal{L}_{FL} &= -\alpha_t (1 - p_t)^\gamma \log(p_t)
		\end{align}
	\end{minipage}
\end{center}

The model demonstrates high performance in classifying the Wake and N3 sleep stages, achieving correct classification rates of 97.14\% and 97.94\%, respectively. In contrast, the N2 stage is accurately classified 88.3\% of the time, though it shows some confusion with neighboring N1 and REM stages. Misclassifications are more prominent for the N1 and REM stages, with the REM stage frequently misidentified as N1, exhibiting a 17.97\% confusion rate. These results indicate strong general performance by the model, though enhancements may be beneficial for improving distinction between the N1 and REM stages.

\begin{figure}[htbp]
	\centering
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/confusion matrix percentatge wise}
		\caption{Confusion matrix (percentage)}
		\label{fig:confusion-matrix-percentatge-wise}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/confusion matrix samples}
		\caption{Confusion matrix (samples)}
		\label{fig:confusion-matrix-samples}
	\end{minipage}
\end{figure}

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/loss plot}
		\caption{Loss plot}
		\label{fig:loss-plot}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/accuracy plot}
		\caption{Accuracy plot}
		\label{fig:accuracy-plot}
	\end{minipage}
\end{figure}

The class weight distribution used during training is as follows: 0: 1.1923, 1: 5.8152, 2: 2.4719, 3: 4.8223, 4: 4.0506. These values reflect adjustments to counter class imbalance and ensure fairer learning across all sleep stages.

The progression of model performance across selected epochs is summarized below:

\begin{figure}[htbp]
	\centering
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/3D acc loss plot}
		\caption{3D Accuracy vs. Loss}
		\label{fig:3d-acc-loss-plot}
	\end{minipage}
\end{figure}


The sample distribution plot reveals a nearly normal distribution of data across the various sleep stages, confirming a balanced dataset. For the test phase, approximately 1,050 samples were utilized, with 210 samples assigned to each class, ensuring equitable and unbiased performance evaluation.

\begin{figure}[htbp]
	\centering
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/sample distribution plot pdf}
		\caption{Sample distribution}
		\label{fig:sample-distribution-plot-pdf}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/auc vs roc curve}
		\caption{AUC vs. ROC Curve}
		\label{fig:auc-vs-roc-curve}
	\end{minipage}
\end{figure}

\begin{figure}[htbp]
	\centering
	\begin{minipage}[b]{0.31\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/f1_score_plot}
		\caption{F1 Score}
		\label{fig:f1scoreplot}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.31\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/precision_plot}
		\caption{Precision}
		\label{fig:precisionplot}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.31\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/recall_plot}
		\caption{Recall}
		\label{fig:recallplot}
	\end{minipage}
\end{figure}















\subsection{XAI Visualization with Heatmap and Plots}





This section presents the distribution of sleep stage data, which approximates a normal distribution. The dataset is well-balanced, with an equal number of samples across all sleep stages. For testing purposes, approximately 1,050 samples were selected, ensuring that each sleep stage class contains around 210 samples. This balanced distribution supports a fair and unbiased evaluation of the model's performance.





\begin{figure}[H]
	\centering
	\includegraphics[width=0.7\linewidth]{"img/paper_3/shap important"}
	\caption{Bar plot illustrating EEG channel importance based on SHAP values. Higher SHAP values indicate greater influence on the model's sleep stage predictions.}
	
	\label{fig:shap-important}
\end{figure}




\subsubsection*{Interpretable Sleep Stage Classification: Hypnogram Transitions and Feature Importance using LIME and SHAP}

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{"img/paper_3/stage n1"}
		\caption{Stage N1}
		\label{fig:stage-n1}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{"img/paper_3/stage n2"}
		\caption{Stage N2}
		\label{fig:stage-n2}
	\end{minipage}
\end{figure}




To improve the interpretability of the sleep stage classification model, we employ Explainable AI (XAI) techniques. Initially, we analyze the hypnogram transition plot, which illustrates the temporal sequence of sleep stages. This visualization helps assess whether the model's predictions align with the natural and expected progression of sleep cycles.


\begin{figure}[htbp]
	\centering
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{"img/paper_3/stage n3"}
		\caption{Stage N3}
		\label{fig:stage-n3}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{"img/paper_3/stage rem"}
		\caption{Stage REM}
		\label{fig:stage-rem}
	\end{minipage}
\end{figure}



Subsequently, we utilize LIME (Local Interpretable Model-Agnostic Explanations) and SHAP (SHapley Additive exPlanations) to investigate feature importance. These tools provide both global and local explanations of the model's behavior. Specifically, they identify the EEG channels that significantly influence the model's decisions, offering insight into which features are most critical in sleep stage classification.




\begin{figure}[htbp]
	\centering
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{"img/paper_3/stage wake"}
		\caption{Stage Wake}
		\label{fig:stage-wake}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.45\linewidth}
		\includegraphics[width=\linewidth]{img/paper_3/XAI4}
		\caption{XAI Visualization}
		\label{fig:xai4}
	\end{minipage}
\end{figure}


By leveraging these interpretability techniques, we ensure that the model's predictions are not only accurate but also aligned with known physiological sleep dynamics, thereby enhancing trust and transparency in the model's outcomes.









































\subsection{Time Complexity Analysis}

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.40\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/paper_3/preprocessing_on³}
		\caption{Preprocessing (\(O(MN^3 + KEN + MN)\))}
		\label{fig:preprocessingon3}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.40\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/paper_3/training_loop_ot·…}
		\caption{Training Loop (\(O(T(LN + 2N^2D/10^4 + DC))\))}
		\label{fig:trainingloopot}
	\end{minipage}
\end{figure}

\noindent Preprocessing: \(O(MN^3 + KEN + MN)\) \\
Training Loop: \(O(T(LN + 2N^2D/10^4 + DC))\)

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.40\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/paper_3/gcn_layers_on}
		\caption{GCN Layers (\(O(LN)\))}
		\label{fig:gcnlayerson}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.40\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/paper_3/transformer_encoder_on²}
		\caption{Transformer Encoder (\(O(N^2D)\))}
		\label{fig:transformerencoderon2}
	\end{minipage}
\end{figure}

\noindent GCN Layers: \(O(LN)\) \\
Transformer Encoder: \(O(N^2D)\)

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.40\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/paper_3/evaluation_oe_val·…}
		\caption{Evaluation Loop (\(O(E_{\text{val}}(LN + 2N^2D/10^4 + DC))\))}
		\label{fig:evaluationoeval}
	\end{minipage}
	\hfill
	\begin{minipage}[b]{0.40\linewidth}
		\centering
		\includegraphics[width=\linewidth]{img/paper_3/fully_connected_layer_o1}
		\caption{Fully Connected Layer (\(O(DC)\))}
		\label{fig:fullyconnectedlayero1}
	\end{minipage}
\end{figure}

\noindent Evaluation Loop: \(O(E_{\text{val}}(LN + 2N^2D/10^4 + DC))\) \\
Fully Connected Layer: \(O(DC)\)